{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b058e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e0dfb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]]], dtype=uint8),\n",
       " {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CarRacing-v2\", continuous=False)\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b98214eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward with random actions after 30 episodes: -55.54395207731279\n",
      "CPU times: total: 3.36 s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "total_reward = 0\n",
    "number_of_episodes = 30\n",
    "\n",
    "for x in range(number_of_episodes):\n",
    "    episode_reward = 0\n",
    "    while(True):\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if terminated or truncated:\n",
    "            env.reset()\n",
    "            break\n",
    "    total_reward += episode_reward\n",
    "\n",
    "env.close()\n",
    "print(f\"Average reward with random actions after {number_of_episodes} episodes: {total_reward / number_of_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c70a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocess(img):\n",
    "    img = img[:84, 6:90] # CarRacing-v2-specific cropping\n",
    "    # img = cv2.resize(img, dsize=(84, 84)) # or you can simply use rescaling\n",
    "    \n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51132a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEnv(gym.Wrapper):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        skip_frames=4,\n",
    "        stack_frames=4,\n",
    "        initial_no_op=50,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(ImageEnv, self).__init__(env, **kwargs)\n",
    "        self.initial_no_op = initial_no_op\n",
    "        self.skip_frames = skip_frames\n",
    "        self.stack_frames = stack_frames\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset the original environment.\n",
    "        s, info = self.env.reset()\n",
    "\n",
    "        # Do nothing for the next `self.initial_no_op` steps\n",
    "        for i in range(self.initial_no_op):\n",
    "            s, r, terminated, truncated, info = self.env.step(0)\n",
    "        \n",
    "        # Convert a frame to 84 X 84 gray scale one\n",
    "        s = preprocess(s)\n",
    "\n",
    "        # The initial observation is simply a copy of the frame `s`\n",
    "        self.stacked_state = np.tile(s, (self.stack_frames, 1, 1))  # [4, 84, 84]\n",
    "        return self.stacked_state, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        # We take an action for self.skip_frames steps\n",
    "        reward = 0\n",
    "        for _ in range(self.skip_frames):\n",
    "            s, r, terminated, truncated, info = self.env.step(action)\n",
    "            reward += r\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        # Convert a frame to 84 X 84 gray scale one\n",
    "        s = preprocess(s)\n",
    "\n",
    "        # Push the current frame `s` at the end of self.stacked_state\n",
    "        self.stacked_state = np.concatenate((self.stacked_state[1:], s[np.newaxis]), axis=0)\n",
    "\n",
    "        return self.stacked_state, reward, terminated, truncated, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a93fd325",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNActionValue(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, activation=F.relu):\n",
    "        super(CNNActionValue, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(state_dim, 16, kernel_size=8, stride=4)  # [N, 4, 84, 84] -> [N, 16, 20, 20]\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=4, stride=2)  # [N, 16, 20, 20] -> [N, 32, 9, 9]\n",
    "        self.in_features = 32 * 9 * 9\n",
    "        self.fc1 = nn.Linear(self.in_features, 256)\n",
    "        self.fc2 = nn.Linear(256, action_dim)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view((-1, self.in_features))\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f67485e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "print(device) # let's see what device we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a8d25755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dim = (4, 84, 84)\n",
    "action_dim = env.action_space.n\n",
    "model = CNNActionValue(state_dim[0], action_dim).to( device )\n",
    "model.load_state_dict( torch.load(\"dqn_grayscale_890.8579.pt\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "e131c0c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward dqn_grayscale_890.8579.pt model after 30 episodes: 845.4126234815643\n",
      "CPU times: total: 18.5 s\n",
      "Wall time: 5min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eval_env = gym.make('CarRacing-v2', continuous=False, render_mode='rgb_array')\n",
    "eval_env = ImageEnv(eval_env)\n",
    "\n",
    "total_reward = 0\n",
    "number_of_episodes = 30\n",
    "\n",
    "for x in range(number_of_episodes):\n",
    "    frames = []\n",
    "    episode_reward = 0\n",
    "    (s, _), done = eval_env.reset(), False\n",
    "    while not done:\n",
    "        frames.append(eval_env.render())\n",
    "        x = torch.from_numpy(s).float().unsqueeze(0).to(device)\n",
    "        q = model(x)\n",
    "        a = torch.argmax(q).item()\n",
    "        observation, reward, terminated, truncated, info = eval_env.step(a)\n",
    "        s = observation\n",
    "        episode_reward += reward\n",
    "        done = terminated or truncated\n",
    "    total_reward += episode_reward\n",
    "\n",
    "env.close()\n",
    "print(f\"Average reward dqn_grayscale_890.8579.pt model after {number_of_episodes} episodes: {total_reward / number_of_episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b59b67e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(imgs, video_name, _return=True):\n",
    "    import cv2\n",
    "    import os\n",
    "    import string\n",
    "    import random\n",
    "    \n",
    "    if video_name is None:\n",
    "        video_name = ''.join(random.choice(string.ascii_letters) for i in range(18)) + '.webm'\n",
    "    height, width, layers = imgs[0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'VP90')\n",
    "    video = cv2.VideoWriter(video_name, fourcc, 10, (width, height))\n",
    "    \n",
    "    for img in imgs:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        video.write(img)\n",
    "    video.release()\n",
    "    if _return:\n",
    "        from IPython.display import Video\n",
    "        return Video(video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f42859e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "episode_reward = 0\n",
    "(s, _), done = eval_env.reset(), False\n",
    "while not done:\n",
    "    frames.append(eval_env.render())\n",
    "    x = torch.from_numpy(s).float().unsqueeze(0).to(device)\n",
    "    q = model(x)\n",
    "    a = torch.argmax(q).item()\n",
    "    observation, reward, terminated, truncated, info = eval_env.step(a)\n",
    "    s = observation\n",
    "    episode_reward += reward\n",
    "    done = terminated or truncated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a08fab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "823.6440677965977\n"
     ]
    }
   ],
   "source": [
    "print(episode_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "ca9a956d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"test1.webm\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "animate(frames, 'test1.webm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5b0bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
